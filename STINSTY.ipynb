{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Introduction to the problem/task and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen dataset for this machine project is the [FIFA Players Dataset](https://www.kaggle.com/datasets/maso0dahmed/football-players-data). The original dataset contains a total of 17954 rows and 51 features. The features contain identifying data (eg. name, birth_data), performance metrics (eg. overall_rating, freekick_accuracy) and other player-related statistics (eg. balance, agression, stamina). The dataset was created and sourced by Masood Ahmed and Talha Turab and is free to use in Kaggle. The data was scraped by the authors from [SoFIFA.com](https://sofifa.com/), a FIFA player statistic site spanning the various FIFA games.\n",
    "\n",
    "The models aim to predict a players overall rating given relevant features. Therefore, this problem classifies as a regression task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Description of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset only includes one csv file. Each of the \\~18k rows in the dataset represents a FIFA Player. Their 51 columns represent the dataset features. These features include (feature descriptions sourced from the dataset authors, see [here](https://www.kaggle.com/datasets/maso0dahmed/football-players-data/data#:~:text=and%20career%20progressions.-,Features%3A,-name%3A%20Name%20of):\n",
    "\n",
    "## Features\n",
    "\n",
    "- **name**: Name of the player.\n",
    "- **full_name**: Full name of the player.\n",
    "- **birth_date**: Date of birth of the player.\n",
    "- **age**: Age of the player.\n",
    "- **height_cm**: Player's height in centimeters.\n",
    "- **weight_kgs**: Player's weight in kilograms.\n",
    "- **positions**: Positions the player can play.\n",
    "- **nationality**: Player's nationality.\n",
    "- **overall_rating**: Overall rating of the player in FIFA.\n",
    "- **potential**: Potential rating of the player in FIFA.\n",
    "- **value_euro**: Market value of the player in euros.\n",
    "- **wage_euro**: Weekly wage of the player in euros.\n",
    "- **preferred_foot**: Player's preferred foot.\n",
    "- **international_reputation**(1-5): International reputation rating from 1 to 5.\n",
    "- **weak_foot**(1-5): Rating of the player's weaker foot from 1 to 5.\n",
    "- **skill_moves**(1-5): Skill moves rating from 1 to 5.\n",
    "- **body_type**: Player's body type.\n",
    "- **release_clause_euro**: Release clause of the player in euros.\n",
    "- **national_team**: National team of the player.\n",
    "- **national_rating**: Rating in the national team.\n",
    "- **national_team_position**: Position in the national team.\n",
    "- **national_jersey_number**: Jersey number in the national team.\n",
    "- **crossing**: Rating for crossing ability.\n",
    "- **finishing**: Rating for finishing ability.\n",
    "- **heading_accuracy**: Rating for heading accuracy.\n",
    "- **short_passing**: Rating for short passing ability.\n",
    "- **volleys**: Rating for volleys.\n",
    "- **dribbling**: Rating for dribbling.\n",
    "- **curve**: Rating for curve shots.\n",
    "- **freekick_accuracy**: Rating for free kick accuracy.\n",
    "- **long_passing**: Rating for long passing.\n",
    "- **ball_control**: Rating for ball control.\n",
    "- **acceleration**: Rating for acceleration.\n",
    "- **sprint_speed**: Rating for sprint speed.\n",
    "- **agility**: Rating for agility.\n",
    "- **reactions**: Rating for reactions.\n",
    "- **balance**: Rating for balance.\n",
    "- **shot_power**: Rating for shot power.\n",
    "- **jumping**: Rating for jumping.\n",
    "- **stamina**: Rating for stamina.\n",
    "- **strength**: Rating for strength.\n",
    "- **long_shots**: Rating for long shots.\n",
    "- **aggression**: Rating for aggression.\n",
    "- **interceptions**: Rating for interceptions.\n",
    "- **positioning**: Rating for positioning.\n",
    "- **vision**: Rating for vision.\n",
    "- **penalties**: Rating for penalties.\n",
    "- **composure**: Rating for composure.\n",
    "- **marking**: Rating for marking.\n",
    "- **standing_tackle**: Rating for standing tackle.\n",
    "- **sliding_tackle**: Rating for sliding tackle.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. List of requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these libraries serve specific purposes and are commonly used when working with statistics and machine learning projects. Pandas for example allows us to transform data into Dataframe and Series data structures and lets us manipulate the data within really easily. Matplotlib and Seaborn are excellent graphing libraries while Scikit-learn lets us easily train and imrpove AI and machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of external python libraries\n",
    "1. Pandas\n",
    "2. Numpy\n",
    "3. Matplotlib\n",
    "4. Seaborn\n",
    "5. Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Data preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Importing all relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Brief overview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17954 entries, 0 to 17953\n",
      "Data columns (total 51 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   name                           17954 non-null  object \n",
      " 1   full_name                      17954 non-null  object \n",
      " 2   birth_date                     17954 non-null  object \n",
      " 3   age                            17954 non-null  int64  \n",
      " 4   height_cm                      17954 non-null  float64\n",
      " 5   weight_kgs                     17954 non-null  float64\n",
      " 6   positions                      17954 non-null  object \n",
      " 7   nationality                    17954 non-null  object \n",
      " 8   overall_rating                 17954 non-null  int64  \n",
      " 9   potential                      17954 non-null  int64  \n",
      " 10  value_euro                     17699 non-null  float64\n",
      " 11  wage_euro                      17708 non-null  float64\n",
      " 12  preferred_foot                 17954 non-null  object \n",
      " 13  international_reputation(1-5)  17954 non-null  int64  \n",
      " 14  weak_foot(1-5)                 17954 non-null  int64  \n",
      " 15  skill_moves(1-5)               17954 non-null  int64  \n",
      " 16  body_type                      17954 non-null  object \n",
      " 17  release_clause_euro            16117 non-null  float64\n",
      " 18  national_team                  857 non-null    object \n",
      " 19  national_rating                857 non-null    float64\n",
      " 20  national_team_position         857 non-null    object \n",
      " 21  national_jersey_number         857 non-null    float64\n",
      " 22  crossing                       17954 non-null  int64  \n",
      " 23  finishing                      17954 non-null  int64  \n",
      " 24  heading_accuracy               17954 non-null  int64  \n",
      " 25  short_passing                  17954 non-null  int64  \n",
      " 26  volleys                        17954 non-null  int64  \n",
      " 27  dribbling                      17954 non-null  int64  \n",
      " 28  curve                          17954 non-null  int64  \n",
      " 29  freekick_accuracy              17954 non-null  int64  \n",
      " 30  long_passing                   17954 non-null  int64  \n",
      " 31  ball_control                   17954 non-null  int64  \n",
      " 32  acceleration                   17954 non-null  int64  \n",
      " 33  sprint_speed                   17954 non-null  int64  \n",
      " 34  agility                        17954 non-null  int64  \n",
      " 35  reactions                      17954 non-null  int64  \n",
      " 36  balance                        17954 non-null  int64  \n",
      " 37  shot_power                     17954 non-null  int64  \n",
      " 38  jumping                        17954 non-null  int64  \n",
      " 39  stamina                        17954 non-null  int64  \n",
      " 40  strength                       17954 non-null  int64  \n",
      " 41  long_shots                     17954 non-null  int64  \n",
      " 42  aggression                     17954 non-null  int64  \n",
      " 43  interceptions                  17954 non-null  int64  \n",
      " 44  positioning                    17954 non-null  int64  \n",
      " 45  vision                         17954 non-null  int64  \n",
      " 46  penalties                      17954 non-null  int64  \n",
      " 47  composure                      17954 non-null  int64  \n",
      " 48  marking                        17954 non-null  int64  \n",
      " 49  standing_tackle                17954 non-null  int64  \n",
      " 50  sliding_tackle                 17954 non-null  int64  \n",
      "dtypes: float64(7), int64(35), object(9)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df = pd.read_csv(\"./fifa_players.csv\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>age</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kgs</th>\n",
       "      <th>positions</th>\n",
       "      <th>nationality</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>potential</th>\n",
       "      <th>...</th>\n",
       "      <th>long_shots</th>\n",
       "      <th>aggression</th>\n",
       "      <th>interceptions</th>\n",
       "      <th>positioning</th>\n",
       "      <th>vision</th>\n",
       "      <th>penalties</th>\n",
       "      <th>composure</th>\n",
       "      <th>marking</th>\n",
       "      <th>standing_tackle</th>\n",
       "      <th>sliding_tackle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L. Messi</td>\n",
       "      <td>Lionel Andrés Messi Cuccittini</td>\n",
       "      <td>6/24/1987</td>\n",
       "      <td>31</td>\n",
       "      <td>170.18</td>\n",
       "      <td>72.1</td>\n",
       "      <td>CF,RW,ST</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>94</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>75</td>\n",
       "      <td>96</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C. Eriksen</td>\n",
       "      <td>Christian  Dannemann Eriksen</td>\n",
       "      <td>2/14/1992</td>\n",
       "      <td>27</td>\n",
       "      <td>154.94</td>\n",
       "      <td>76.2</td>\n",
       "      <td>CAM,RM,CM</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>88</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>46</td>\n",
       "      <td>56</td>\n",
       "      <td>84</td>\n",
       "      <td>91</td>\n",
       "      <td>67</td>\n",
       "      <td>88</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P. Pogba</td>\n",
       "      <td>Paul Pogba</td>\n",
       "      <td>3/15/1993</td>\n",
       "      <td>25</td>\n",
       "      <td>190.50</td>\n",
       "      <td>83.9</td>\n",
       "      <td>CM,CAM</td>\n",
       "      <td>France</td>\n",
       "      <td>88</td>\n",
       "      <td>91</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>78</td>\n",
       "      <td>64</td>\n",
       "      <td>82</td>\n",
       "      <td>88</td>\n",
       "      <td>82</td>\n",
       "      <td>87</td>\n",
       "      <td>63</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         name                       full_name birth_date  age  height_cm  \\\n",
       "0    L. Messi  Lionel Andrés Messi Cuccittini  6/24/1987   31     170.18   \n",
       "1  C. Eriksen    Christian  Dannemann Eriksen  2/14/1992   27     154.94   \n",
       "2    P. Pogba                      Paul Pogba  3/15/1993   25     190.50   \n",
       "\n",
       "   weight_kgs  positions nationality  overall_rating  potential  ...  \\\n",
       "0        72.1   CF,RW,ST   Argentina              94         94  ...   \n",
       "1        76.2  CAM,RM,CM     Denmark              88         89  ...   \n",
       "2        83.9     CM,CAM      France              88         91  ...   \n",
       "\n",
       "   long_shots  aggression interceptions  positioning  vision  penalties  \\\n",
       "0          94          48            22           94      94         75   \n",
       "1          89          46            56           84      91         67   \n",
       "2          82          78            64           82      88         82   \n",
       "\n",
       "  composure  marking standing_tackle  sliding_tackle  \n",
       "0        96       33              28              26  \n",
       "1        88       59              57              22  \n",
       "2        87       63              67              67  \n",
       "\n",
       "[3 rows x 51 columns]"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See aggregate metrics of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kgs</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>potential</th>\n",
       "      <th>value_euro</th>\n",
       "      <th>wage_euro</th>\n",
       "      <th>international_reputation(1-5)</th>\n",
       "      <th>weak_foot(1-5)</th>\n",
       "      <th>skill_moves(1-5)</th>\n",
       "      <th>...</th>\n",
       "      <th>long_shots</th>\n",
       "      <th>aggression</th>\n",
       "      <th>interceptions</th>\n",
       "      <th>positioning</th>\n",
       "      <th>vision</th>\n",
       "      <th>penalties</th>\n",
       "      <th>composure</th>\n",
       "      <th>marking</th>\n",
       "      <th>standing_tackle</th>\n",
       "      <th>sliding_tackle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>1.769900e+04</td>\n",
       "      <td>17708.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "      <td>17954.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>25.565445</td>\n",
       "      <td>174.946921</td>\n",
       "      <td>75.301047</td>\n",
       "      <td>66.240169</td>\n",
       "      <td>71.430935</td>\n",
       "      <td>2.479280e+06</td>\n",
       "      <td>9902.134628</td>\n",
       "      <td>1.111674</td>\n",
       "      <td>2.945695</td>\n",
       "      <td>2.361034</td>\n",
       "      <td>...</td>\n",
       "      <td>46.852456</td>\n",
       "      <td>55.816531</td>\n",
       "      <td>46.657959</td>\n",
       "      <td>49.857302</td>\n",
       "      <td>53.406260</td>\n",
       "      <td>48.357302</td>\n",
       "      <td>58.680183</td>\n",
       "      <td>47.162861</td>\n",
       "      <td>47.733040</td>\n",
       "      <td>45.705915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.705708</td>\n",
       "      <td>14.029449</td>\n",
       "      <td>7.083684</td>\n",
       "      <td>6.963730</td>\n",
       "      <td>6.131339</td>\n",
       "      <td>5.687014e+06</td>\n",
       "      <td>21995.593750</td>\n",
       "      <td>0.392168</td>\n",
       "      <td>0.663691</td>\n",
       "      <td>0.763223</td>\n",
       "      <td>...</td>\n",
       "      <td>19.429337</td>\n",
       "      <td>17.394047</td>\n",
       "      <td>20.754649</td>\n",
       "      <td>19.694311</td>\n",
       "      <td>14.156038</td>\n",
       "      <td>15.810844</td>\n",
       "      <td>11.625541</td>\n",
       "      <td>20.037346</td>\n",
       "      <td>21.674973</td>\n",
       "      <td>21.285812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>152.400000</td>\n",
       "      <td>49.900000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>154.940000</td>\n",
       "      <td>69.900000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>3.250000e+05</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>175.260000</td>\n",
       "      <td>74.800000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>7.000000e+05</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>185.420000</td>\n",
       "      <td>79.800000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>2.100000e+06</td>\n",
       "      <td>9000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>205.740000</td>\n",
       "      <td>110.200000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>1.105000e+08</td>\n",
       "      <td>565000.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                age     height_cm    weight_kgs  overall_rating     potential  \\\n",
       "count  17954.000000  17954.000000  17954.000000    17954.000000  17954.000000   \n",
       "mean      25.565445    174.946921     75.301047       66.240169     71.430935   \n",
       "std        4.705708     14.029449      7.083684        6.963730      6.131339   \n",
       "min       17.000000    152.400000     49.900000       47.000000     48.000000   \n",
       "25%       22.000000    154.940000     69.900000       62.000000     67.000000   \n",
       "50%       25.000000    175.260000     74.800000       66.000000     71.000000   \n",
       "75%       29.000000    185.420000     79.800000       71.000000     75.000000   \n",
       "max       46.000000    205.740000    110.200000       94.000000     95.000000   \n",
       "\n",
       "         value_euro      wage_euro  international_reputation(1-5)  \\\n",
       "count  1.769900e+04   17708.000000                   17954.000000   \n",
       "mean   2.479280e+06    9902.134628                       1.111674   \n",
       "std    5.687014e+06   21995.593750                       0.392168   \n",
       "min    1.000000e+04    1000.000000                       1.000000   \n",
       "25%    3.250000e+05    1000.000000                       1.000000   \n",
       "50%    7.000000e+05    3000.000000                       1.000000   \n",
       "75%    2.100000e+06    9000.000000                       1.000000   \n",
       "max    1.105000e+08  565000.000000                       5.000000   \n",
       "\n",
       "       weak_foot(1-5)  skill_moves(1-5)  ...    long_shots    aggression  \\\n",
       "count    17954.000000      17954.000000  ...  17954.000000  17954.000000   \n",
       "mean         2.945695          2.361034  ...     46.852456     55.816531   \n",
       "std          0.663691          0.763223  ...     19.429337     17.394047   \n",
       "min          1.000000          1.000000  ...      3.000000     11.000000   \n",
       "25%          3.000000          2.000000  ...     32.000000     44.000000   \n",
       "50%          3.000000          2.000000  ...     51.000000     59.000000   \n",
       "75%          3.000000          3.000000  ...     62.000000     69.000000   \n",
       "max          5.000000          5.000000  ...     94.000000     95.000000   \n",
       "\n",
       "       interceptions   positioning        vision     penalties     composure  \\\n",
       "count   17954.000000  17954.000000  17954.000000  17954.000000  17954.000000   \n",
       "mean       46.657959     49.857302     53.406260     48.357302     58.680183   \n",
       "std        20.754649     19.694311     14.156038     15.810844     11.625541   \n",
       "min         3.000000      2.000000     10.000000      5.000000     12.000000   \n",
       "25%        26.000000     38.000000     44.000000     38.000000     51.000000   \n",
       "50%        52.000000     55.000000     55.000000     49.000000     60.000000   \n",
       "75%        64.000000     64.000000     64.000000     60.000000     67.000000   \n",
       "max        92.000000     95.000000     94.000000     92.000000     96.000000   \n",
       "\n",
       "            marking  standing_tackle  sliding_tackle  \n",
       "count  17954.000000     17954.000000    17954.000000  \n",
       "mean      47.162861        47.733040       45.705915  \n",
       "std       20.037346        21.674973       21.285812  \n",
       "min        3.000000         2.000000        3.000000  \n",
       "25%       30.000000        27.000000       24.000000  \n",
       "50%       52.500000        55.000000       52.000000  \n",
       "75%       64.000000        66.000000       64.000000  \n",
       "max       94.000000        93.000000       90.000000  \n",
       "\n",
       "[8 rows x 42 columns]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we drop the `potential` column. This is because the goal of our model is to predict a new rating and compare how accurate it is to the `overall_rating`. Since the potential rating of a player is almost 1:1 correlated with this output, we decided not to use the feature and instead predict only based on other features like skill-related features, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop('potential', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for null and missing values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                                 0\n",
       "full_name                            0\n",
       "birth_date                           0\n",
       "age                                  0\n",
       "height_cm                            0\n",
       "weight_kgs                           0\n",
       "positions                            0\n",
       "nationality                          0\n",
       "overall_rating                       0\n",
       "value_euro                         255\n",
       "wage_euro                          246\n",
       "preferred_foot                       0\n",
       "international_reputation(1-5)        0\n",
       "weak_foot(1-5)                       0\n",
       "skill_moves(1-5)                     0\n",
       "body_type                            0\n",
       "release_clause_euro               1837\n",
       "national_team                    17097\n",
       "national_rating                  17097\n",
       "national_team_position           17097\n",
       "national_jersey_number           17097\n",
       "crossing                             0\n",
       "finishing                            0\n",
       "heading_accuracy                     0\n",
       "short_passing                        0\n",
       "volleys                              0\n",
       "dribbling                            0\n",
       "curve                                0\n",
       "freekick_accuracy                    0\n",
       "long_passing                         0\n",
       "ball_control                         0\n",
       "acceleration                         0\n",
       "sprint_speed                         0\n",
       "agility                              0\n",
       "reactions                            0\n",
       "balance                              0\n",
       "shot_power                           0\n",
       "jumping                              0\n",
       "stamina                              0\n",
       "strength                             0\n",
       "long_shots                           0\n",
       "aggression                           0\n",
       "interceptions                        0\n",
       "positioning                          0\n",
       "vision                               0\n",
       "penalties                            0\n",
       "composure                            0\n",
       "marking                              0\n",
       "standing_tackle                      0\n",
       "sliding_tackle                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean our dataset, we first drop all duplicate values using `drop_duplicates` then columns with high null values with `drop_na`. Additionally, we will drop columns that act as unique identifiers for rows since they will not help our model's training. We will also drop rows with null `value_euro` or `wage_euro` values since there aren't a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'255 rows were dropped'"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_length = len(df)\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates(keep='first', inplace=False)\n",
    "\n",
    "# Drop rows with null 'value_euro' or 'wage_euro'\n",
    "df = df.dropna(subset=[\"value_euro\", \"wage_euro\"])\n",
    "\n",
    "# Drop columns with high null values\n",
    "df = df.drop(columns=[\"release_clause_euro\", \"national_team\", \"national_rating\", \"national_team_position\", \"national_jersey_number\"])\n",
    "\n",
    "# Drop identifier columns\n",
    "df = df.drop(columns=['name', 'full_name', 'birth_date'])\n",
    "\n",
    "# Reset index to adjust the row indices\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "new_length = len(df)\n",
    "\n",
    "# Show how many rows were dropped\n",
    "str(old_length - new_length) + \" rows were dropped\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the player continents we binned earlier, check to see if there are any values with too little rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4d. Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look through dataset for multiple representations and to understand the values better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to 'dumps/unique_values.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('dumps', exist_ok=True)\n",
    "\n",
    "# Open the text file in write mode with utf-8 encoding\n",
    "with open('dumps/unique_values.txt', 'w', encoding='utf-8') as file:\n",
    "    for column in df.columns:\n",
    "        # print(column + \" column is being written\")\n",
    "        values = sorted(df[column].unique())\n",
    "        file.write(\"Column: \" + column + \"\\n\")\n",
    "        file.write(\"Unique Values = \" + \", \".join(map(str, values)) + \"\\n\")\n",
    "        file.write(str(len(values)) + \" unique values\\n\\n\")\n",
    "        # print(column + \" has been written\")\n",
    "print(\"Data exported to 'dumps/unique_values.txt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We bin the data using the `nationality` column and group rows by their continent. This is since some countries only have a very few amount of rows associated with them, which may introduce imbalance to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_continent = {\n",
    "    'Afghanistan': 'Asia', 'Albania': 'Europe', 'Algeria': 'Africa', 'Andorra': 'Europe', \n",
    "    'Angola': 'Africa', 'Antigua & Barbuda': 'North America', 'Argentina': 'South America', \n",
    "    'Armenia': 'Asia', 'Australia': 'Oceania', 'Austria': 'Europe', 'Azerbaijan': 'Asia', \n",
    "    'Barbados': 'North America', 'Belarus': 'Europe', 'Belgium': 'Europe', 'Benin': 'Africa', \n",
    "    'Bermuda': 'North America', 'Bolivia': 'South America', 'Bosnia Herzegovina': 'Europe', \n",
    "    'Brazil': 'South America', 'Bulgaria': 'Europe', 'Burkina Faso': 'Africa', 'Burundi': 'Africa', \n",
    "    'Cameroon': 'Africa', 'Canada': 'North America', 'Cape Verde': 'Africa', 'Central African Rep.': 'Africa',\n",
    "    'Chad': 'Africa', 'Chile': 'South America', 'China PR': 'Asia', 'Colombia': 'South America', \n",
    "    'Comoros': 'Africa', 'Congo': 'Africa', 'Costa Rica': 'North America', 'Croatia': 'Europe', \n",
    "    'Cuba': 'North America', 'Curacao': 'North America', 'Cyprus': 'Europe', 'Czech Republic': 'Europe', \n",
    "    'DR Congo': 'Africa', 'Denmark': 'Europe', 'Dominican Republic': 'North America', 'Ecuador': 'South America', \n",
    "    'Egypt': 'Africa', 'El Salvador': 'North America', 'England': 'Europe', 'Equatorial Guinea': 'Africa', \n",
    "    'Eritrea': 'Africa', 'Estonia': 'Europe', 'Ethiopia': 'Africa', 'FYR Macedonia': 'Europe', 'Faroe Islands': 'Europe', \n",
    "    'Fiji': 'Oceania', 'Finland': 'Europe', 'France': 'Europe', 'Gabon': 'Africa', 'Gambia': 'Africa', \n",
    "    'Georgia': 'Asia', 'Germany': 'Europe', 'Ghana': 'Africa', 'Greece': 'Europe', 'Grenada': 'North America', \n",
    "    'Guam': 'Oceania', 'Guatemala': 'North America', 'Guinea': 'Africa', 'Guinea Bissau': 'Africa', \n",
    "    'Guyana': 'South America', 'Haiti': 'North America', 'Honduras': 'North America', 'Hong Kong': 'Asia', \n",
    "    'Hungary': 'Europe', 'Iceland': 'Europe', 'Indonesia': 'Asia', 'Iran': 'Asia', 'Iraq': 'Asia', \n",
    "    'Israel': 'Asia', 'Italy': 'Europe', 'Ivory Coast': 'Africa', 'Jamaica': 'North America', 'Japan': 'Asia', \n",
    "    'Jordan': 'Asia', 'Kazakhstan': 'Asia', 'Kenya': 'Africa', 'Korea DPR': 'Asia', 'Korea Republic': 'Asia', \n",
    "    'Kosovo': 'Europe', 'Kuwait': 'Asia', 'Latvia': 'Europe', 'Liberia': 'Africa', 'Libya': 'Africa', \n",
    "    'Liechtenstein': 'Europe', 'Lithuania': 'Europe', 'Luxembourg': 'Europe', 'Madagascar': 'Africa', \n",
    "    'Mali': 'Africa', 'Malta': 'Europe', 'Mauritania': 'Africa', 'Mexico': 'North America', 'Moldova': 'Europe', \n",
    "    'Montenegro': 'Europe', 'Montserrat': 'North America', 'Morocco': 'Africa', 'Mozambique': 'Africa', \n",
    "    'Namibia': 'Africa', 'Netherlands': 'Europe', 'New Caledonia': 'Oceania', 'New Zealand': 'Oceania', \n",
    "    'Nicaragua': 'North America', 'Nigeria': 'Africa', 'Northern Ireland': 'Europe', 'Norway': 'Europe', \n",
    "    'Oman': 'Asia', 'Palestine': 'Asia', 'Panama': 'North America', 'Papua New Guinea': 'Oceania', \n",
    "    'Paraguay': 'South America', 'Peru': 'South America', 'Philippines': 'Asia', 'Poland': 'Europe', \n",
    "    'Portugal': 'Europe', 'Republic of Ireland': 'Europe', 'Romania': 'Europe', 'Russia': 'Europe', \n",
    "    'Rwanda': 'Africa', 'Saudi Arabia': 'Asia', 'Scotland': 'Europe', 'Senegal': 'Africa', \n",
    "    'Serbia': 'Europe', 'Sierra Leone': 'Africa', 'Slovakia': 'Europe', 'Slovenia': 'Europe', \n",
    "    'South Africa': 'Africa', 'South Sudan': 'Africa', 'Spain': 'Europe', 'St Kitts Nevis': 'North America', \n",
    "    'St Lucia': 'North America', 'Sudan': 'Africa', 'Suriname': 'South America', 'Sweden': 'Europe', \n",
    "    'Switzerland': 'Europe', 'Syria': 'Asia', 'São Tomé & Príncipe': 'Africa', 'Tanzania': 'Africa', \n",
    "    'Thailand': 'Asia', 'Togo': 'Africa', 'Trinidad & Tobago': 'North America', 'Tunisia': 'Africa', \n",
    "    'Turkey': 'Asia', 'Uganda': 'Africa', 'Ukraine': 'Europe', 'United Arab Emirates': 'Asia', \n",
    "    'United States': 'North America', 'Uruguay': 'South America', 'Uzbekistan': 'Asia', 'Venezuela': 'South America', \n",
    "    'Vietnam': 'Asia', 'Wales': 'Europe', 'Yemen': 'Asia', 'Zambia': 'Africa', 'Zimbabwe': 'Africa'\n",
    "}\n",
    "\n",
    "# Map each nationality to its continent\n",
    "df['continent'] = df['nationality'].map(country_to_continent)\n",
    "\n",
    "# Check the result\n",
    "df = df.drop('nationality', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we one-hot-encode the player positions. There are 15 total positions in the one-hot-vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CAM</th>\n",
       "      <th>CB</th>\n",
       "      <th>CDM</th>\n",
       "      <th>CF</th>\n",
       "      <th>CM</th>\n",
       "      <th>GK</th>\n",
       "      <th>LB</th>\n",
       "      <th>LM</th>\n",
       "      <th>LW</th>\n",
       "      <th>LWB</th>\n",
       "      <th>RB</th>\n",
       "      <th>RM</th>\n",
       "      <th>RW</th>\n",
       "      <th>RWB</th>\n",
       "      <th>ST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CAM  CB  CDM  CF  CM  GK  LB  LM  LW  LWB  RB  RM  RW  RWB  ST\n",
       "0    0   0    0   1   0   0   0   0   0    0   0   0   1    0   1\n",
       "1    1   0    0   0   1   0   0   0   0    0   0   1   0    0   0\n",
       "2    1   0    0   0   1   0   0   0   0    0   0   0   0    0   0"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions_split = df['positions'].str.get_dummies(sep=',')\n",
    "positions_split.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine these columns with the dataframe and get rid of the `positions` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, positions_split], axis=1)\n",
    "df = df.drop(columns=['positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'height_cm', 'weight_kgs', 'overall_rating', 'value_euro',\n",
       "       'wage_euro', 'preferred_foot', 'international_reputation(1-5)',\n",
       "       'weak_foot(1-5)', 'skill_moves(1-5)', 'body_type', 'crossing',\n",
       "       'finishing', 'heading_accuracy', 'short_passing', 'volleys',\n",
       "       'dribbling', 'curve', 'freekick_accuracy', 'long_passing',\n",
       "       'ball_control', 'acceleration', 'sprint_speed', 'agility', 'reactions',\n",
       "       'balance', 'shot_power', 'jumping', 'stamina', 'strength', 'long_shots',\n",
       "       'aggression', 'interceptions', 'positioning', 'vision', 'penalties',\n",
       "       'composure', 'marking', 'standing_tackle', 'sliding_tackle',\n",
       "       'continent', 'CAM', 'CB', 'CDM', 'CF', 'CM', 'GK', 'LB', 'LM', 'LW',\n",
       "       'LWB', 'RB', 'RM', 'RW', 'RWB', 'ST'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "continent\n",
       "Europe           10405\n",
       "South America     3040\n",
       "Asia              1916\n",
       "Africa            1176\n",
       "North America      892\n",
       "Oceania            270\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['continent'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values seem fine, so we will label encode the continents to make them easier to work with for the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continents:  \n",
    "> 0 - Africa  \n",
    "1 - Asia  \n",
    "2 - Europe  \n",
    "3 - North America  \n",
    "4 - Oceania  \n",
    "5 - South America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['continent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "df[\"continent\"] = le.transform(df[\"continent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See transformed `continent` rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "continent\n",
       "2    10405\n",
       "5     3040\n",
       "1     1916\n",
       "0     1176\n",
       "3      892\n",
       "4      270\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['continent'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label encode the body types as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['body_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(labels)\n",
    "df['body_type'] = le.transform(df[\"body_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body_type\n",
       "6    10223\n",
       "3     6400\n",
       "9     1069\n",
       "4        1\n",
       "2        1\n",
       "7        1\n",
       "0        1\n",
       "8        1\n",
       "5        1\n",
       "1        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we do the same with preferred foot (left, right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preferred_foot\n",
       "1    13579\n",
       "0     4120\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df['preferred_foot'].unique()\n",
    "le.fit(labels)\n",
    "df['preferred_foot'] = le.transform(df[\"preferred_foot\"])\n",
    "df['preferred_foot'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4e. Invalid rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be getting the features that should have values in range of 0 to 100 to check for any invalid data in these features. They will be stored in the `skill_100` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acceleration</th>\n",
       "      <th>jumping</th>\n",
       "      <th>aggression</th>\n",
       "      <th>dribbling</th>\n",
       "      <th>short_passing</th>\n",
       "      <th>positioning</th>\n",
       "      <th>reactions</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>long_shots</th>\n",
       "      <th>freekick_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>penalties</th>\n",
       "      <th>balance</th>\n",
       "      <th>strength</th>\n",
       "      <th>ball_control</th>\n",
       "      <th>interceptions</th>\n",
       "      <th>long_passing</th>\n",
       "      <th>composure</th>\n",
       "      <th>sliding_tackle</th>\n",
       "      <th>shot_power</th>\n",
       "      <th>sprint_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>68</td>\n",
       "      <td>48</td>\n",
       "      <td>97</td>\n",
       "      <td>92</td>\n",
       "      <td>94</td>\n",
       "      <td>95</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>75</td>\n",
       "      <td>95</td>\n",
       "      <td>66</td>\n",
       "      <td>96</td>\n",
       "      <td>22</td>\n",
       "      <td>89</td>\n",
       "      <td>96</td>\n",
       "      <td>26</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76</td>\n",
       "      <td>50</td>\n",
       "      <td>46</td>\n",
       "      <td>84</td>\n",
       "      <td>91</td>\n",
       "      <td>84</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>89</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>81</td>\n",
       "      <td>58</td>\n",
       "      <td>91</td>\n",
       "      <td>56</td>\n",
       "      <td>89</td>\n",
       "      <td>88</td>\n",
       "      <td>22</td>\n",
       "      <td>84</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>83</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>88</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>66</td>\n",
       "      <td>87</td>\n",
       "      <td>90</td>\n",
       "      <td>64</td>\n",
       "      <td>90</td>\n",
       "      <td>87</td>\n",
       "      <td>67</td>\n",
       "      <td>90</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   acceleration  jumping  aggression  dribbling  short_passing  positioning  \\\n",
       "0            91       68          48         97             92           94   \n",
       "1            76       50          46         84             91           84   \n",
       "2            71       83          78         87             86           82   \n",
       "\n",
       "   reactions  overall_rating  long_shots  freekick_accuracy  ...  penalties  \\\n",
       "0         95              94          94                 94  ...         75   \n",
       "1         88              88          89                 87  ...         67   \n",
       "2         82              88          82                 82  ...         82   \n",
       "\n",
       "   balance  strength  ball_control  interceptions  long_passing  composure  \\\n",
       "0       95        66            96             22            89         96   \n",
       "1       81        58            91             56            89         88   \n",
       "2       66        87            90             64            90         87   \n",
       "\n",
       "   sliding_tackle  shot_power  sprint_speed  \n",
       "0              26          85            86  \n",
       "1              22          84            73  \n",
       "2              67          90            79  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_100 = df[['acceleration', 'jumping', 'aggression', 'dribbling', 'short_passing', 'positioning', 'reactions', 'overall_rating', 'long_shots', 'freekick_accuracy', 'volleys', 'vision', 'crossing', 'curve', 'agility', 'finishing', 'heading_accuracy', 'standing_tackle', 'marking', 'stamina', 'penalties', 'balance', 'strength', 'ball_control', 'interceptions', 'long_passing', 'composure', 'sliding_tackle', 'shot_power', 'sprint_speed']]\n",
    "skill_100.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, these features are all well within their 0-100 range, meaning we don't have to prune any invalid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid rows: 0\n"
     ]
    }
   ],
   "source": [
    "invalid_rows = skill_100[(skill_100 < 1).any(axis=1) | (skill_100 > 100).any(axis=1)]\n",
    "\n",
    "print(\"Number of invalid rows:\", len(invalid_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will store all columns with values whose ranges are not limited to 0-100 in `not_skill`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'height_cm', 'weight_kgs', 'value_euro', 'wage_euro',\n",
       "       'preferred_foot', 'international_reputation(1-5)', 'weak_foot(1-5)',\n",
       "       'skill_moves(1-5)', 'body_type', 'continent', 'CAM', 'CB', 'CDM', 'CF',\n",
       "       'CM', 'GK', 'LB', 'LM', 'LW', 'LWB', 'RB', 'RM', 'RW', 'RWB', 'ST'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_skill = df.drop(columns=skill_100.columns)\n",
    "not_skill.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seperate `not_skill` into two dataframes containing continuous and categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_skill_int = not_skill[['age', 'height_cm', 'weight_kgs', 'value_euro', 'wage_euro']]\n",
    "not_skill_cat = not_skill.drop(columns=not_skill_int.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets outliers outside the percentile range of a certain column\n",
    "def get_outliers(col, percentile):\n",
    "    alpha = 1 - percentile\n",
    "    percentile_5 = df[col].quantile(alpha / 2)\n",
    "    percentile_95 = df[col].quantile(1 - (alpha/2))\n",
    "    \n",
    "    in_range = df[col][(df[col] >= percentile_5) & (df[col] <= percentile_95)]\n",
    "    outliers = df[col][(df[col] <= percentile_5) | (df[col] >= percentile_95)]\n",
    "    print(f\"Column: {col}, Outliers: {len(outliers)}, Inlier Range: {percentile_5} - {percentile_95}, Value Range: {df[col].min()} - {df[col].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See outliers and their ranges for continious columns in `not_skill_int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: age, Outliers: 1388, Inlier Range: 18.0 - 35.0, Value Range: 17 - 46\n",
      "Column: height_cm, Outliers: 3506, Inlier Range: 152.4 - 193.04, Value Range: 152.4 - 205.74\n",
      "Column: weight_kgs, Outliers: 993, Inlier Range: 62.1 - 89.8, Value Range: 49.9 - 110.2\n",
      "Column: value_euro, Outliers: 985, Inlier Range: 70000.0 - 15500000.0, Value Range: 10000.0 - 110500000.0\n",
      "Column: wage_euro, Outliers: 5292, Inlier Range: 1000.0 - 58000.0, Value Range: 1000.0 - 565000.0\n"
     ]
    }
   ],
   "source": [
    "for col in not_skill_int.columns:\n",
    "    get_outliers(col, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum and minimum values for these features aren't human errors, so we will not be removing any outliers and will rely on normalization instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns in the not_skill_cat dataframe are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['preferred_foot', 'international_reputation(1-5)', 'weak_foot(1-5)',\n",
       "       'skill_moves(1-5)', 'body_type', 'continent', 'CAM', 'CB', 'CDM', 'CF',\n",
       "       'CM', 'GK', 'LB', 'LM', 'LW', 'LWB', 'RB', 'RM', 'RW', 'RWB', 'ST'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_skill_cat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, for `body_type` we check to see if there are any values with too little rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body_type\n",
       "6    10223\n",
       "3     6400\n",
       "9     1069\n",
       "4        1\n",
       "2        1\n",
       "7        1\n",
       "0        1\n",
       "8        1\n",
       "5        1\n",
       "1        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_skill_cat.body_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the rows containing these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body_type\n",
       "1    10223\n",
       "0     6400\n",
       "2     1069\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['body_type'].isin([6, 3, 9])]\n",
    "le = LabelEncoder()\n",
    "le.fit(df['body_type'].unique())\n",
    "df['body_type'] = le.transform(df['body_type'])\n",
    "df.body_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4f. Removing highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "X = df.drop('overall_rating', axis=1)\n",
    "y = df['overall_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping standing_tackle (0.268) over sliding_tackle (0.256)\n",
      "Dropping ball_control (0.444) over dribbling (0.437)\n",
      "Dropping interceptions (0.258) over sliding_tackle (0.240)\n",
      "Dropping acceleration (0.345) over sprint_speed (0.333)\n",
      "Dropping marking (0.251) over sliding_tackle (0.227)\n",
      "Dropping dribbling (0.425) over positioning (0.407)\n",
      "Dropping short_passing (0.396) over long_passing (0.358)\n",
      "Dropping long_shots (0.386) over shot_power (0.373)\n",
      "Dropping positioning (0.379) over finishing (0.354)\n",
      "Dropping volleys (0.347) over finishing (0.342)\n",
      "Dropping curve (0.356) over freekick_accuracy (0.330)\n",
      "Dropping value_euro (0.153) over wage_euro (0.141)\n",
      "Dropping finishing (0.321) over penalties (0.310)\n",
      "Dropping shot_power (0.320) over penalties (0.298)\n",
      "Dropping GK (0.284) over heading_accuracy (0.239)\n",
      "Dropping agility (0.290) over balance (0.262)\n",
      "Dropping crossing (0.316) over freekick_accuracy (0.284)\n",
      "Dropping freekick_accuracy (0.272) over penalties (0.258)\n",
      "Dropping aggression (0.232) over sliding_tackle (0.208)\n",
      "Dropping long_passing (0.259) over vision (0.255)\n",
      "Dropping skill_moves(1-5) (0.247) over penalties (0.233)\n"
     ]
    }
   ],
   "source": [
    "# threshhold for correlaton\n",
    "thresh = 0.7\n",
    "fcorr = 1\n",
    "\n",
    "# if highest correlation between features is less than threshold, break.\n",
    "while fcorr >= thresh:\n",
    "    # unstack correlation matrix\n",
    "    X_corr = X.corr().abs()\n",
    "    uns = X_corr.unstack().reset_index()\n",
    "    \n",
    "    # drop duplicates and identity rows, as well as sort by highest\n",
    "    uns.drop_duplicates(inplace=True)\n",
    "    uns.sort_values(0, inplace=True, ascending=False)\n",
    "    # uns = uns[uns['level_0'] != uns['level_1']]\n",
    "    uns = uns[uns[0] != 1.0]\n",
    "    \n",
    "    # unpack feature 1, feature 2 and corr\n",
    "    f1, f2, fcorr = uns.iloc[0][:]\n",
    "\n",
    "    # drop f1, f2 from correlations\n",
    "    f1_corr = pd.DataFrame(X_corr[f1]).drop([f1, f2])\n",
    "    f2_corr = pd.DataFrame(X_corr[f2]).drop([f1, f2])\n",
    "\n",
    "    # mean of other correlated features > threshhold\n",
    "    # f1_s = f1_corr[f1_corr[f1] > thresh].mean().iloc[0]\n",
    "    # f2_s = f2_corr[f2_corr[f2] > thresh].mean().iloc[0]\n",
    "\n",
    "    # number of other correlated features > thresh\n",
    "    # f1_s = len(f1_corr[f1_corr[f1] > thresh])\n",
    "    # f2_s = len(f2_corr[f2_corr[f2] > thresh])\n",
    "\n",
    "    # mean of all other correlated features\n",
    "    f1_s = f1_corr.mean().iloc[0]\n",
    "    f2_s = f2_corr.mean().iloc[0]\n",
    "\n",
    "    # drop feature with highest metric\n",
    "    if f1_s > f2_s:\n",
    "        X.drop(columns=[f1], inplace=True)\n",
    "        print(f'Dropping {f1} ({f1_s:.3f}) over {f2} ({f2_s:.3f})')\n",
    "    else:\n",
    "        X.drop(columns=[f2], inplace=True)\n",
    "        print(f'Dropping {f2} ({f2_s:.3f}) over {f1} ({f1_s:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = X.corr().abs().unstack().reset_index().sort_values(0, ascending=False)\n",
    "new_X = new_X[new_X[0] != 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'height_cm', 'weight_kgs', 'wage_euro', 'preferred_foot',\n",
      "       'international_reputation(1-5)', 'weak_foot(1-5)', 'body_type',\n",
      "       'heading_accuracy', 'sprint_speed', 'reactions', 'balance', 'jumping',\n",
      "       'stamina', 'strength', 'vision', 'penalties', 'composure',\n",
      "       'sliding_tackle', 'continent', 'CAM', 'CB', 'CDM', 'CF', 'CM', 'LB',\n",
      "       'LM', 'LW', 'LWB', 'RB', 'RM', 'RW', 'RWB', 'ST'],\n",
      "      dtype='object')\n",
      "TOTAL COLUMNS LEFT: 34, COLUMNDS DROPPED: 22\n"
     ]
    }
   ],
   "source": [
    "print(X.columns)\n",
    "print(f'TOTAL COLUMNS LEFT: {len(X.columns)}, COLUMNDS DROPPED: {len(df.columns) - len(X.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4g. Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = list(set(df.columns) - set(['name','full_name','positions',\n",
    "                                        'potential', 'preferred_foot', \n",
    "                                        'weak_foot', 'body_type','nationality'\n",
    "                                        ,'international_reputation(1-5)'\n",
    "                                        ,'birth_date','height_cm','weight_kgs',\n",
    "                                        'height_cm', 'weight_kg', 'continent', 'value_bin'\n",
    "                                        ,'Category', \"skill_moves(1-5)\", \"weak_foot(1-5)\",\n",
    "                                        'overall_rating']))\n",
    "\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Check for missing values in numeric and categorical columns\n",
    "print(\"Missing values in numeric features:\")\n",
    "print(df[numeric_features].isna().sum())\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(df[numeric_features])\n",
    "new_df = pd.DataFrame(scaled_features, columns=numeric_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['overall_rating']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([new_df, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Are there any correlations within our X values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a. Overall correlation within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = new_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "correlation_matrix = new_df[numeric_columns].corr()\n",
    "\n",
    "formatted_matrix = correlation_matrix.applymap(lambda x: f\"{x:.3f}\")\n",
    "\n",
    "print(formatted_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,8))\n",
    "plot = sns.heatmap(correlation_matrix, fmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_with_wage= df.corr(numeric_only=True)['value_euro']\n",
    "correlation_with_wage = correlation_with_wage.sort_values(ascending=False)\n",
    "print(correlation_with_wage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unf = correlation_matrix.unstack()\n",
    "so = unf.sort_values(kind=\"quicksort\").drop_duplicates()\n",
    "so = so[::-1]\n",
    "so[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unf = correlation_matrix.unstack()\n",
    "so = unf.sort_values(kind=\"quicksort\").drop_duplicates()\n",
    "so = so[:20] \n",
    "so\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = new_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = new_df[numeric_columns].corr()\n",
    "\n",
    "# Get correlations with the target variable 'overall_rating'\n",
    "target_correlations = correlation_matrix['overall_rating'].sort_values(ascending=False)\n",
    "\n",
    "# Format and print correlations\n",
    "formatted_correlations = target_correlations.apply(lambda x: f\"{x:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_with_target = correlation_matrix[['overall_rating']].sort_values(by='overall_rating', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(5, len(correlations_with_target) // 2)) \n",
    "sns.heatmap(correlations_with_target, annot=True, fmt=\".3f\", cmap='coolwarm', cbar=True)\n",
    "\n",
    "plt.title(\"Correlation with overall_rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlations_with_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. PCA to remove certain correlations within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_matrix = np.cov(X.T)\n",
    "\n",
    "covariance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(eigenvalues)[::-1] \n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio = sorted_eigenvalues / np.sum(sorted_eigenvalues)\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance vs. Number of Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_check = new_df.drop('overall_rating', axis=1)\n",
    "\n",
    "optimal_k =5\n",
    "\n",
    "pca = PCA(n_components=optimal_k)  \n",
    "X_pca = pca.fit_transform(pca_check)\n",
    "\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "loadings = pca.components_\n",
    "\n",
    "df_loadings = pd.DataFrame(loadings.T, columns=[f'PC{i+1}' for i in range(loadings.shape[0])], index=pca_check.columns)\n",
    "\n",
    "df_loadings['abs_loading'] = df_loadings['PC1'].abs()\n",
    "df_loadings = df_loadings.sort_values('abs_loading', ascending=False)\n",
    "\n",
    "print(\"\\nSorted Feature Loadings (Top Features Based on PC1):\")\n",
    "print(df_loadings)\n",
    "\n",
    "top_features = df_loadings.head(optimal_k).index\n",
    "print(f\"\\nTop {optimal_k} Features based on PCA loadings:\", top_features)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance vs. Number of Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping all the valuable columns and checking for new correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df = pd.concat([new_df], axis=1)\n",
    "eda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_df), len(y), len(eda_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data correlation with respect to PCA columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(pd.concat([eda_df[top_features], y], axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_df), len(y), len(eda_df))\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = pca_df.corr()['overall_rating'].drop('overall_rating')\n",
    "\n",
    "# Display the correlations\n",
    "print(correlations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correlations = pca_df.corr()\n",
    "\n",
    "# Extract the correlation of all features with 'overall_rating' and reshape to 2D\n",
    "correlations_with_overall = correlations[['overall_rating']]\n",
    "\n",
    "# Set up the plot size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(correlations_with_overall, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Correlation Heatmap with Overall Rating')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Overall Rating')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How is the data distributed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. How is the general distribution of data whilst respecting PCA results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in pca_df.columns:\n",
    "    plt.figure(figsize=(24, 8))\n",
    "    sns.histplot(x=pca_df[column], label=f'distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. How is the distribution of low variance data with respect to the overall_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in pca_df.columns:\n",
    "\n",
    "    try:\n",
    "        plt.figure(figsize=(20, 16))\n",
    "        sns.boxplot(data=pca_df, x='overall_rating', y=column)\n",
    "        plt.title(f\"Boxplot for 'overall_rating' with respect to '{column}'\")\n",
    "        plt.xlabel('Overall Rating')\n",
    "        plt.ylabel('column')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c. How is the distribution of highly correlated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = eda_df[['reactions', 'composure', 'value_euro',  'overall_rating']]\n",
    "\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in corr_df.columns:\n",
    "    try:\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        sns.histplot(x=corr_df[column], label=f'distribution of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.title(f'Distribution of {column}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2d. How correlated is the highley correlated data with respect to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = corr_df.corr()\n",
    "\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)\n",
    "\n",
    "plt.title('Correlation Matrix for Selected Features')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Features')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2e. boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in corr_df.columns:\n",
    "\n",
    "    try:\n",
    "        plt.figure(figsize=(20, 16))\n",
    "        sns.boxplot(data=corr_df, x='overall_rating', y=column)\n",
    "        plt.title(f\"Boxplot for 'overall_rating' with respect to '{column}'\")\n",
    "        plt.xlabel('Overall Rating')\n",
    "        plt.ylabel('column')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How does the data look like with respect to the position a player plays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Bin general positions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_based_on_first_match(positions):\n",
    "    # Split the positions into a list\n",
    "    position_list = positions.split(',')\n",
    "    \n",
    "    # Iterate through positions and assign category based on the first match\n",
    "    for position in position_list:\n",
    "        if position in position_to_category:\n",
    "            return position_to_category[position]\n",
    "    \n",
    "    return 'Unknown'  # In case no position matches\n",
    "# Define bins for positions\n",
    "bins = {\n",
    "    \"Attacking\": ['CF', 'ST', 'LW', 'RW'],\n",
    "    \"Midfield\": ['CAM', 'CM', 'CDM', 'LM', 'RM'],\n",
    "    \"Defense\": ['CB', 'LB', 'RB', 'LWB', 'RWB'],\n",
    "    \"Goalkeeper\": ['GK']\n",
    "}\n",
    "\n",
    "# Reverse map positions to categories\n",
    "position_to_category = {}\n",
    "for category, positions in bins.items():\n",
    "    for position in positions:\n",
    "        position_to_category[position] = category\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "new_df['Category'] = df['positions'].apply(assign_based_on_first_match)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in new_df.columns:\n",
    "    plt.figure(figsize=(20, 16))\n",
    "\n",
    "    # Boxplot for 'overall_rating'\n",
    "    sns.boxplot(data=new_df, x='Category', y=col)\n",
    "    plt.title(f\"Boxplot for 'Category' with respect to '{col}'\")\n",
    "    plt.xlabel('Overall Rating')\n",
    "    plt.ylabel('Value (Euro)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6. Initial model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: *Insert reason why we use this model here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the train and test datasets using train_test_split. We set test_size to 0.3 as it is a pretty standard test size. We stratify based on y to ensure that the data is split evently between the classes. Shuffle is to help randomize the data for possibly better fitting. Random state set for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_knn = normalized_df\n",
    "y_knn = normalized_df.values[:, -2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize features using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_knn = MinMaxScaler().fit_transform(X_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_knn, y_knn, test_size=0.3, stratify=y_knn, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "# X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# rus = RandomUnderSampler(random_state=42)\n",
    "# X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# from imblearn.combine import SMOTETomek\n",
    "# smt = SMOTETomek(random_state=42)\n",
    "# X_train, y_train = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See train and test split shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup KNN Classifier and start fitting to the training data. For now, we will choose as arbitrary K value and distance metric that doesn't perform too badly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=20, metric=\"euclidean\", weights=\"uniform\")\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model on training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = knn.predict(X_train)\n",
    "knn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a \"decent\" accuracy on training data, let's check testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we test on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = knn.predict(X_test)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a similar accuracy on testing data, this means no overfitting or underfitting is occuring. Unfortunately, the accuracies aren't high in general, meaning the model isn't too useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, y_pred_train, labels=knn.classes_)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)\n",
    "cmd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test, labels=knn.classes_)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)\n",
    "cmd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[163], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m(y_test, y_pred_test))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_values = [1, 5, 10, 20, 40, 50, 60, 70, 80, 100, 200, 500, 1000]\n",
    "\n",
    "best_n = None\n",
    "best_cv_score = 0\n",
    "best_t_score = 0\n",
    "t_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "print(\"Tuning hyperparameters with cross-validation...\")\n",
    "for n_neighbors in neighbor_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric=\"euclidean\", weights=\"distance\")\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(knn, X_train, y_train, cv=5, scoring=\"accuracy\"))\n",
    "    t_score = np.mean(cross_val_score(knn, X_test, y_test, cv=5, scoring=\"accuracy\"))\n",
    "\n",
    "    cv_scores.append(cv_score)\n",
    "    t_scores.append(t_score)\n",
    "    \n",
    "    if cv_score > best_cv_score:\n",
    "        best_cv_score = cv_score\n",
    "        best_n = n_neighbors\n",
    "\n",
    "    if t_score > best_t_score:\n",
    "        best_t_score = t_score\n",
    "\n",
    "    print(f\"n_neighbors={n_neighbors}, CV Accuracy={cv_score:.4f}, T Accuracy = {t_score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest number of neighbors: {best_n} with CV Accuracy: {best_cv_score:.4f}\")\n",
    "final_knn = KNeighborsClassifier(n_neighbors=best_n, metric=\"euclidean\", weights=\"distance\")\n",
    "final_knn.fit(X_train, y_train)\n",
    "\n",
    "test_score = final_knn.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neighbor_values, cv_scores, marker='o')\n",
    "plt.xlabel(\"Number of Neighbors (n_neighbors)\")\n",
    "plt.ylabel(\"Cross-Validation Accuracy\")\n",
    "plt.title(\"Hyperparameter Tuning Results\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_values = [1, 5, 10, 20,40, 50, 60, 70, 80, 100, 200, 500, 1000]\n",
    "\n",
    "best_n = None\n",
    "best_cv_score = 0\n",
    "best_t_score = 0\n",
    "t_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "print(\"Tuning hyperparameters with cross-validation...\")\n",
    "for n_neighbors in neighbor_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric=\"euclidean\", weights=\"uniform\")\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(knn, X_train, y_train, cv=5, scoring=\"accuracy\"))\n",
    "    t_score = np.mean(cross_val_score(knn, X_test, y_test, cv=5, scoring=\"accuracy\"))\n",
    "\n",
    "    cv_scores.append(cv_score)\n",
    "    t_scores.append(t_score)\n",
    "    \n",
    "    if cv_score > best_cv_score:\n",
    "        best_cv_score = cv_score\n",
    "        best_n = n_neighbors\n",
    "\n",
    "    if t_score > best_t_score:\n",
    "        best_t_score = t_score\n",
    "\n",
    "    print(f\"n_neighbors={n_neighbors}, CV Accuracy={cv_score:.4f}, T Accuracy = {t_score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest number of neighbors: {best_n} with CV Accuracy: {best_cv_score:.4f}\")\n",
    "final_knn = KNeighborsClassifier(n_neighbors=best_n, metric=\"euclidean\", weights=\"uniform\")\n",
    "final_knn.fit(X_train, y_train)\n",
    "\n",
    "test_score = final_knn.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neighbor_values, cv_scores, marker='o')\n",
    "plt.xlabel(\"Number of Neighbors (n_neighbors)\")\n",
    "plt.ylabel(\"Cross-Validation Accuracy\")\n",
    "plt.title(\"Hyperparameter Tuning Results\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dt = box_df_clean.drop(\"Class\",axis=1)\n",
    "y_dt = box_df_clean[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dt, y_dt, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Training data label counts:\")\n",
    "print(np.array([unique, counts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"Test data label counts:\")\n",
    "print(np.array([unique, counts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = [\n",
    "   {\n",
    "      'criterion': ['gini', 'entropy'],           \n",
    "      'max_depth': [5, 10, 20, 30],               \n",
    "      'min_samples_split': [2, 4, 6, 10, 15, 20, 30, 50],  \n",
    "      'max_leaf_nodes': [3, 5, 10, 20, 50, 100, 200, 500]\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsc_df = RandomizedSearchCV(estimator=dtc, \n",
    "                         param_distributions=hyperparameters,\n",
    "                         n_iter=50,\n",
    "                         cv=5,\n",
    "                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsc_df.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "rsc_results = pd.DataFrame(rsc_df.cv_results_)\n",
    "rsc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = rsc_df.best_index_\n",
    "best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsc_results.loc[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = rsc_df.best_score_\n",
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtc.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, actual):\n",
    "   return sum(predictions == actual) / len(actual) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy: \", compute_accuracy(predictions, y_train),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing accuracy: \", compute_accuracy(predictions, y_test),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predictions)\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "cmd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_tree(clf):\n",
    "    n_nodes = clf.tree_.node_count\n",
    "    children_left = clf.tree_.children_left\n",
    "    children_right = clf.tree_.children_right\n",
    "    feature = clf.tree_.feature\n",
    "    threshold = clf.tree_.threshold\n",
    "    values = clf.tree_.value\n",
    "\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "    while len(stack) > 0:\n",
    "        # `pop` ensures each node is only visited once\n",
    "        node_id, depth = stack.pop()\n",
    "        node_depth[node_id] = depth\n",
    "\n",
    "        # If the left and right child of a node is not the same we have a split\n",
    "        # node\n",
    "        is_split_node = children_left[node_id] != children_right[node_id]\n",
    "        # If a split node, append left and right children and depth to `stack`\n",
    "        # so we can loop through them\n",
    "        if is_split_node:\n",
    "            stack.append((children_left[node_id], depth + 1))\n",
    "            stack.append((children_right[node_id], depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "\n",
    "    print(\n",
    "        \"The binary tree structure has {n} nodes and has \"\n",
    "        \"the following tree structure:\\n\".format(n=n_nodes)\n",
    "    )\n",
    "    for i in range(n_nodes):\n",
    "        if is_leaves[i]:\n",
    "            print(\n",
    "                \"{space}node={node} is a leaf node, values: {values}.\".format(\n",
    "                    space=node_depth[i] * \"\\t\", node=i, values=values[i]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"{space}node={node} is a split node: \"\n",
    "                \"go to node {left} if X[:, {feature}] <= {threshold} \"\n",
    "                \"else to node {right}.\".format(\n",
    "                    space=node_depth[i] * \"\\t\",\n",
    "                    node=i,\n",
    "                    left=children_left[i],\n",
    "                    feature=feature[i],\n",
    "                    threshold=threshold[i],\n",
    "                    right=children_right[i],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_tree(dtc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model with respect to the `corr_df` df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corr_df.drop([\"overall_rating\"],axis=1)\n",
    "y = corr_df[\"overall_rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lin_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred_test, color='blue', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Perfect prediction line\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model with respect to the `pca_df` df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pca_df.drop([\"overall_rating\"],axis=1)\n",
    "y = pca_df[\"overall_rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lin_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(f'Mean Squared Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred_test, color='blue', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Perfect prediction line\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using regularization techniques to reduce impace from correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ridge` regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming corr_df is your DataFrame\n",
    "X = corr_df.drop([\"overall_rating\"], axis=1)\n",
    "y = corr_df[\"overall_rating\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Ridge regression model\n",
    "ridge_reg = Ridge(alpha=1.0)  # Adjust alpha for regularization strength\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_train = ridge_reg.predict(X_train)\n",
    "y_pred = ridge_reg.predict(X_test)\n",
    "\n",
    "# Clip the predictions to stay within the range [0, 100]\n",
    "y_pred_train = np.clip(y_pred_train, 0, 100)\n",
    "y_pred = np.clip(y_pred, 0, 100)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "# Print performance metrics\n",
    "print(f'Mean Squared Error (Training): {mse_train}')\n",
    "print(f'Mean Squared Error (Testing): {mse}')\n",
    "print(f'R² Score (Train): {r2_train}')\n",
    "print(f'R² Score (Testing): {r2}')\n",
    "\n",
    "# Visualize Actual vs Predicted for Testing\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Perfect prediction line\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted (Ridge Regression TESTING)')\n",
    "plt.show()\n",
    "\n",
    "# Visualize Actual vs Predicted for Training\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_train, y_pred_train, color='blue', alpha=0.6)\n",
    "plt.plot([y_train.min(), y_train.max()], [y_pred_train.min(), y_pred_train.max()], 'r--')  # Perfect prediction line\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted (Ridge Regression TRAINING)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lasso` regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # Import numpy for np.clip\n",
    "\n",
    "# Assuming corr_df is your DataFrame\n",
    "X = corr_df.drop([\"overall_rating\"], axis=1)\n",
    "y = corr_df[\"overall_rating\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Lasso regression model\n",
    "lasso_reg = Lasso(alpha=0.001)  # Adjust alpha for regularization strength\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_train = lasso_reg.predict(X_train)\n",
    "y_pred = lasso_reg.predict(X_test)\n",
    "\n",
    "# Clip the predictions to stay within the range [0, 100]\n",
    "y_pred_train = np.clip(y_pred_train, 0, 100)\n",
    "y_pred = np.clip(y_pred, 0, 100)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "# Print performance metrics\n",
    "print(f'Mean Squared Error (Training): {mse_train}')\n",
    "print(f'Mean Squared Error (Testing): {mse}')\n",
    "print(f'R² Score (Train): {r2_train}')\n",
    "print(f'R² Score (Testing): {r2}')\n",
    "\n",
    "# Visualize Actual vs Predicted for Testing\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Perfect prediction line\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted (Lasso Regression TESTING)')\n",
    "plt.show()\n",
    "\n",
    "# Visualize Actual vs Predicted for Training\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_train, y_pred_train, color='blue', alpha=0.6)\n",
    "plt.plot([y_train.min(), y_train.max()], [y_pred_train.min(), y_pred_train.max()], 'r--')  # Perfect prediction line\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted (Lasso Regression TRAINING)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using hyperparameter tuning to adjust the `alpha` of the regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ridge` regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define the Ridge model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define the parameter grid for alpha\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='r2', cv=5, return_train_score=True)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and make predictions\n",
    "best_ridge_model = grid_search.best_estimator_\n",
    "\n",
    "# Clip the predictions after fitting\n",
    "y_pred_train = best_ridge_model.predict(X_train)\n",
    "y_pred_test = best_ridge_model.predict(X_test)\n",
    "\n",
    "# Clip the predictions to stay within the range [0, 100]\n",
    "y_pred_train = np.clip(y_pred_train, 0, 100)\n",
    "y_pred_test = np.clip(y_pred_test, 0, 100)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(f'Mean Squared Error (Training): {mse_train}')\n",
    "print(f'Mean Squared Error (Testing): {mse_test}')\n",
    "print(f'R² Score (Train): {r2_train}')\n",
    "print(f'R² Score (Testing): {r2_test}')\n",
    "\n",
    "# Extract results from GridSearchCV\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Print the full grid search results\n",
    "print(\"Grid Search Results:\")\n",
    "print(results[['param_alpha', 'mean_test_score', 'mean_train_score']])\n",
    "\n",
    "# Extract values for plotting\n",
    "alphas = param_grid['alpha']\n",
    "mean_test_scores = results['mean_test_score']\n",
    "mean_train_scores = results['mean_train_score']\n",
    "\n",
    "# Plot alpha vs. R² score\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(alphas, mean_test_scores, marker='o', linestyle='--', color='b', label='Mean Test Score')\n",
    "plt.plot(alphas, mean_train_scores, marker='s', linestyle='-', color='r', label='Mean Train Score')\n",
    "plt.xscale('log')  # Use log scale for alpha\n",
    "plt.xlabel('Alpha (Regularization Strength)')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Effect of Alpha on R² Score (Ridge Regression)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Best alpha and score\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "best_r2 = grid_search.best_score_\n",
    "print(f'\\nBest alpha: {best_alpha}')\n",
    "print(f'Best cross-validated R² score: {best_r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lasso` regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Lasso model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Define the parameter grid for alpha\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=lasso, param_grid=param_grid, scoring='r2', cv=5, return_train_score=True)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Print the full grid search results\n",
    "print(\"Grid Search Results:\")\n",
    "print(results[['param_alpha', 'mean_test_score', 'mean_train_score', ]])\n",
    "\n",
    "# Extract values for plotting\n",
    "alphas = param_grid['alpha']\n",
    "mean_test_scores = results['mean_test_score']\n",
    "mean_train_scores = results['mean_train_score']\n",
    "\n",
    "# Plot alpha vs. R² score\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(alphas, mean_test_scores, marker='o', linestyle='--', color='b', label='Mean Test Score')\n",
    "plt.plot(alphas, mean_train_scores, marker='s', linestyle='-', color='r', label='Mean Train Score')\n",
    "plt.xscale('log')  # Use log scale for alpha\n",
    "plt.xlabel('Alpha (Regularization Strength)')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Effect of Alpha on R² Score (Lasso Regression)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Best alpha and score\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "best_r2 = grid_search.best_score_\n",
    "print(f'\\nBest alpha: {best_alpha}')\n",
    "print(f'Best cross-validated R² score: {best_r2}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
